{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6739d35c-b8bb-4042-8010-403b0b05cd19",
   "metadata": {},
   "source": [
    "# Proof-of-concept forecasting: predict daily PM2.5 in Lucknow\n",
    "#### _Models: Linear Regression and Random Forest_\n",
    "#### _Outputs: metrics + multiple diagnostic plots & Insights + Predictions & Recommendation_\n",
    "#### _Author: AetherAI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cdcf1-d9b7-498a-a7ca-e6ac1cc1b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", font_scale=1.0)\n",
    "plt.rcParams[\"figure.dpi\"] = 140"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b0433-aa3e-41bd-9055-9bedf39492d6",
   "metadata": {},
   "source": [
    "### 1) Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c7156-d504-40d7-8fef-2d4c4e7a3758",
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "file_path = os.path.join(desktop, \"ML Lucknow.csv\")\n",
    "df = pd.read_csv(file_path, parse_dates=[\"date\"])\n",
    "\n",
    "# Sanity: ensure expected columns exist\n",
    "if \"pm25\" not in df.columns:\n",
    "    raise ValueError(\"Column 'pm25' not found. Ensure your pivot produced a 'pm25' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f7070-edf6-4f13-9b0d-30f59adff3f4",
   "metadata": {},
   "source": [
    "### 2) Clean + feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2896441-2e43-4663-a92d-82b2e6553e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Forward/backward fill short gaps within available periods (keeps the POC simple)\n",
    "df_ffill = df.copy()\n",
    "df_ffill = df_ffill.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "# Basic lag features for pm25 (useful for day-to-day persistence)\n",
    "df_ffill[\"pm25_lag1\"] = df_ffill[\"pm25\"].shift(1)\n",
    "df_ffill[\"pm25_lag7\"] = df_ffill[\"pm25\"].shift(7)\n",
    "df_ffill[\"pm25_rolling7\"] = df_ffill[\"pm25\"].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "# Dropping the first few rows with NaNs introduced by lagging (keeps training clean)\n",
    "df_model = df_ffill.dropna(subset=[\"pm25_lag1\", \"pm25_lag7\", \"pm25_rolling7\"]).copy()\n",
    "\n",
    "# Features: all pollutants except target + lag features\n",
    "feature_cols = [c for c in df_model.columns if c not in [\"date\", \"pm25\"]]\n",
    "X = df_model[feature_cols]\n",
    "y = df_model[\"pm25\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89cad13-85f6-4ce7-a5da-953e86f05607",
   "metadata": {},
   "source": [
    "### 3) Train/test split (time-based) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94d5c8-3c84-48fd-84ae-8589a5d17bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a proof-of-concept, keeping the last 20% as test without shuffling\n",
    "split_idx = int(len(df_model) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "dates_test = df_model[\"date\"].iloc[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c60020-43df-472a-96a1-3d58c26e4348",
   "metadata": {},
   "source": [
    "### 4) Models: Linear Regression + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a59d9e-75ae-43c3-a2b3-dc3d47bab3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling features for Linear Regression (since RF does not need scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_lr = scaler.fit_transform(X_train)\n",
    "X_test_lr = scaler.transform(X_test)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_lr, y_train)\n",
    "y_pred_lr = linreg.predict(X_test_lr)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ddf36c-c50d-47a5-9816-94639ead1317",
   "metadata": {},
   "source": [
    "### 5) Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f8df4-3f10-441a-8c8e-a998af2098b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    # Guard against division by zero\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    nonzero = y_true != 0\n",
    "    return np.mean(np.abs((y_true[nonzero] - y_pred[nonzero]) / y_true[nonzero])) * 100 if nonzero.any() else np.nan\n",
    "\n",
    "def evaluate_all(y_true, y_pred, name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mp = mape(y_true, y_pred)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MAE  = {mae:.2f}\")\n",
    "    print(f\"  RMSE = {rmse:.2f}\")\n",
    "    print(f\"  R²   = {r2:.3f}\")\n",
    "    print(f\"  MAPE = {mp:.2f}%\")\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"MAPE\": mp}\n",
    "\n",
    "metrics_lr = evaluate_all(y_test, y_pred_lr, \"Linear Regression\")\n",
    "metrics_rf = evaluate_all(y_test, y_pred_rf, \"Random Forest\")\n",
    "\n",
    "# H) Summary metrics saved as CSV\n",
    "#metrics_df = pd.DataFrame([\n",
    "    #{\"model\": \"Linear Regression\", **metrics_lr},\n",
    "    #{\"model\": \"Random Forest\", **metrics_rf}\n",
    "#])\n",
    "#metrics_df.to_csv(os.path.join(out_dir, \"metrics_summary.csv\"), index=False)\n",
    "\n",
    "#print(\"Done. Outputs saved to:\", out_dir)\n",
    "#print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504cdc1b-ba83-40f2-8379-9ee1aa844a25",
   "metadata": {},
   "source": [
    "### 6) Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198dda6-52e5-4161-be87-f46f8d879885",
   "metadata": {},
   "source": [
    "##### A) Actual vs Predicted over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c41a9f-91ea-4ae3-829e-516aec2a19e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_dir = os.path.join(desktop, \"lucknow_poc_outputs\")\n",
    "#os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(dates_test, y_test, label=\"Actual PM2.5\", color=\"black\", linewidth=2)\n",
    "plt.plot(dates_test, y_pred_lr, label=\"Linear Regression\", alpha=0.8)\n",
    "plt.plot(dates_test, y_pred_rf, label=\"Random Forest\", alpha=0.8)\n",
    "plt.title(\"Actual vs Predicted PM2.5 (Test Period)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"PM2.5\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(out_dir, \"actual_vs_predicted.png\"))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4422f-2bbd-4b5d-ab9a-578765cc99d2",
   "metadata": {},
   "source": [
    "##### **_Overall Performance_: Both the Linear Regression and Random Forest models generally follow the trend of the actual PM2.5 values. The predicted values rise and fall at roughly the same times as the actual values.**\n",
    "- **_Random Forest Model (Orange line):_** This model appears to be a more accurate predictor of PM2.5 concentrations during the test period. Its line stays closer to the \"Actual PM2.5\" line (black line), especially at the peaks and troughs, indicating a better fit to the data.\n",
    "- **_Linear Regression Model (Blue line):_** This model's predictions are less accurate than the Random Forest model. The blue line consistently underestimates the peaks and overestimates the troughs. For example, during the peak around March 29th, the Linear Regression model's prediction is significantly lower than the actual value, while the Random Forest model's prediction is very close. Similarly, at the trough around March 30th, the Linear Regression model's prediction is higher than the actual value.\n",
    "**In summary**, the Random Forest model demonstrates superior performance in this test period, capturing the true fluctuations in PM2.5 levels more effectively than the Linear Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ecf43-3e7f-46bc-b3b0-2f739ec47bc3",
   "metadata": {},
   "source": [
    "##### B) Scatter: Actual vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975669bb-a164-4b65-9d3f-f7362fddc5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "sns.scatterplot(x=y_test, y=y_pred_lr, ax=axes[0], color=\"#1f77b4\", alpha=0.7)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", linewidth=1)\n",
    "axes[0].set_title(\"Linear Regression: Actual vs Predicted\")\n",
    "axes[0].set_xlabel(\"Actual PM2.5\")\n",
    "axes[0].set_ylabel(\"Predicted PM2.5\")\n",
    "\n",
    "sns.scatterplot(x=y_test, y=y_pred_rf, ax=axes[1], color=\"#2ca02c\", alpha=0.7)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", linewidth=1)\n",
    "axes[1].set_title(\"Random Forest: Actual vs Predicted\")\n",
    "axes[1].set_xlabel(\"Actual PM2.5\")\n",
    "axes[1].set_ylabel(\"Predicted PM2.5\")\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(out_dir, \"scatter_actual_vs_predicted.png\"))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7502302a-d593-4339-a5bb-5274694b7208",
   "metadata": {},
   "source": [
    "##### **_Scatter Plot Analysis_**\n",
    "Both graphs plot the actual values on the x-axis and the predicted values on the y-axis. The red diagonal line represents a perfect prediction where the predicted value is exactly the same as the actual value. A good model's data points will cluster closely along this line.\n",
    "\n",
    "-  **_Linear Regression:_** The scatter plot shows a linear relationship, with the data points loosely following the red line. However, some points, especially at the higher ends of the scale, show a larger deviation from the line, which suggests the model may not be capturing all the complexities of the data. Linear regression assumes a linear relationship between variables, which may not always be accurate for real-world data.\n",
    "-  **_Random Forest:_** The scatter plot for the Random Forest model shows the data points are more tightly clustered around the red line, with less deviation than the linear regression model. This suggests that the Random Forest model is performing better at predicting the PM2.5 values, likely because it is better at handling complex, non-linear relationships in the data without making assumptions about its distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e40406-143a-48e1-bdb7-d853f5b62369",
   "metadata": {},
   "source": [
    "##### C) Residual histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba41aed-e48f-4855-9cef-43435279a257",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res_lr = y_test - y_pred_lr\n",
    "res_rf = y_test - y_pred_rf\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,5))\n",
    "sns.histplot(res_lr, bins=20, kde=True, ax=axes[0], color=\"#1f77b4\")\n",
    "axes[0].set_title(\"Residuals (Linear Regression)\")\n",
    "axes[0].set_xlabel(\"Error (Actual - Predicted)\")\n",
    "\n",
    "sns.histplot(res_rf, bins=20, kde=True, ax=axes[1], color=\"#2ca02c\")\n",
    "axes[1].set_title(\"Residuals (Random Forest)\")\n",
    "axes[1].set_xlabel(\"Error (Actual - Predicted)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(out_dir, \"residual_histograms.png\"))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471024f-0329-4b90-96e4-57d45e7e3141",
   "metadata": {},
   "source": [
    "##### **_Analysis of the Residual Plots_**\n",
    "- **_Residuals:_** In machine learning, a residual is the difference between an actual data value and the value predicted by the model (Error = Actual - Predicted). Analyzing these residuals helps to evaluate a model's validity and fitness. A good model typically has residuals that are randomly distributed around zero.\n",
    "- **_Residuals (Linear Regression):_** The histogram on the left shows the residuals for a Linear Regression model. The distribution of the errors appears to be centered around zero and roughly follows a normal distribution, which is an assumption of linear regression. The plot indicates a random scattering of residuals, suggesting that the linear model might be a suitable fit for the data.\n",
    "- **_Residuals (Random Forest):_** The histogram on the right shows the residuals for a Random Forest model. The errors are more concentrated around zero, and the distribution appears to be taller and narrower compared to the linear regression plot. This suggests that the Random Forest model's predictions are, on average, closer to the actual values than those of the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2bc34-480f-4dc4-b9d4-7b6a7419a8bc",
   "metadata": {},
   "source": [
    "##### D) Residuals over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bdb532-0aeb-4539-bb87-a2edfda0a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(dates_test, res_lr, label=\"Residuals (LR)\", alpha=0.8)\n",
    "plt.plot(dates_test, res_rf, label=\"Residuals (RF)\", alpha=0.8)\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.title(\"Residuals Over Time (Test Period)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Residual (Actual - Predicted)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(out_dir, \"residuals_over_time.png\"))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8082e0-bf78-4f59-85a2-ddbfe0344258",
   "metadata": {},
   "source": [
    "##### **_Residuals:_** The y-axis represents the \"Residual (Actual - Predicted)\" values. A residual is the difference between a model's predicted value and the actual observed value.\n",
    "\n",
    "- **_Time Series Data:_** The x-axis shows dates, indicating that the analysis is being performed on time-series data.\n",
    "- **_Two Models:_** The plot compares the residuals of two different models:\n",
    "- **_Residuals (LR):_** Likely from a Linear Regression model.\n",
    "- **_Residuals (RF):_** Likely from a Random Forest model.\n",
    "- **_Plot Interpretation:_** Ideally, residuals should be randomly scattered around the zero line with no discernible pattern. A pattern, such as a curve or a systematic trend, suggests that the model is not adequately capturing the relationship in the data. The plot in the image shows how the prediction errors for both models change over time, which helps in diagnosing the adequacy and assumptions of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cdabcd-4491-402e-9e55-ef89f707685b",
   "metadata": {},
   "source": [
    "##### E) QQ-like diagnostic using quantiles (quick check of error normality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf9bd6-511d-4cde-8d74-199ca5428955",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = np.linspace(0.01, 0.99, 50)\n",
    "lr_q = np.quantile(res_lr, quantiles)\n",
    "rf_q = np.quantile(res_rf, quantiles)\n",
    "theory_q = np.quantile(np.random.normal(0, np.std(res_lr), size=100000), quantiles)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(theory_q, lr_q, \"o\", alpha=0.5, label=\"LR residuals\")\n",
    "plt.plot(theory_q, rf_q, \"o\", alpha=0.5, label=\"RF residuals\")\n",
    "plt.plot([theory_q.min(), theory_q.max()], [theory_q.min(), theory_q.max()], \"r--\", linewidth=1)\n",
    "plt.title(\"QQ-style plot of residuals vs normal quantiles\")\n",
    "plt.xlabel(\"Theoretical quantiles\")\n",
    "plt.ylabel(\"Empirical residual quantiles\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(out_dir, \"qq_residuals.png\"))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6090215d-b110-4c3c-9def-94429e193eb2",
   "metadata": {},
   "source": [
    "##### **_Q-Q Plot Analysis_**\n",
    "The plot of **residuals vs normal quantiles**, compares the empirical quantiles of two sets of residuals (from an LR model and an RF model) against the theoretical quantiles of a normal distribution. In this type of plot, if the data follows a normal distribution, the points should align closely with the diagonal reference line. \n",
    "\n",
    "- **_LR residuals:_** The blue data points representing **LR residuals** are generally close to the straight line, suggesting that these residuals are approximately normally distributed.\n",
    "- **_RF residuals:_** The orange data points for **RF residuals** also follow the line, but with more deviation at the extreme ends. This could indicate that the residuals from the Random Forest model may have slightly heavier tails or more extreme values than a normal distribution would predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3cd49-fc98-44e9-a252-de2e860aa686",
   "metadata": {},
   "source": [
    "##### F) Feature importances (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39b82dc-7da4-46ef-aea7-3d47105d7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=importances.values, y=importances.index, color=\"#2ca02c\")\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(os.path.join(out_dir, \"feature_importances_rf.png\"))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7cb2f8-e056-459a-bf30-c2e62bd484d6",
   "metadata": {},
   "source": [
    "##### **_Feature Importance:_** In a Random Forest model, feature importance is a value assigned to each feature that represents how much that feature contributes to the model's predictive power. Features with a higher importance score are considered more significant for making accurate predictions.\n",
    "- **_Interpretation of the Chart:_** The length of each bar corresponds to the importance score of a specific feature. In this chart, the feature with the longest bar has the highest importance.\n",
    "- **_Features:_** The features being analyzed are listed on the y-axis: pm25_rolling7, so2, pm25_lag7, pm25_lag1, no2, and o3. The chart shows that **CO** has the highest importance, followed by **so2** and **pm25_rolling7**. The other features have significantly lower importance scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef29784-357a-4cd6-b150-76fcf2836bd9",
   "metadata": {},
   "source": [
    "##### G) Predicted vs Actual line per model (separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f317e-cc9e-4ac7-ab61-01b3c22fd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, preds, color in [\n",
    "    (\"linear_regression\", y_pred_lr, \"#1f77b4\"),\n",
    "    (\"random_forest\", y_pred_rf, \"#2ca02c\")\n",
    "]:\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(dates_test, y_test, label=\"Actual\", color=\"black\", linewidth=2)\n",
    "    plt.plot(dates_test, preds, label=\"Predicted\", color=color, alpha=0.8)\n",
    "    plt.title(f\"Actual vs Predicted PM2.5 - {name.replace('_', ' ').title()}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"PM2.5\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(os.path.join(out_dir, f\"actual_vs_predicted_{name}.png\"))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a944db-eee3-42c4-b72e-2946f289cdda",
   "metadata": {},
   "source": [
    "##### **_Random Forest Appears to be a Better Fit:_** The Random Forest model's \"predicted\" line (green) more closely follows the \"actual\" line (black) than the Linear Regression model's \"predicted\" line (blue) follows its corresponding \"actual\" line. This indicates that the Random Forest model is likely a better fit for the data, as it is more successful at capturing the fluctuations and trends in the actual PM2.5 values.\n",
    "- **_Linear Regression Underperforms:_** The Linear Regression model's \"predicted\" line is smoother and does not capture the sharp peaks and valleys seen in the actual data. This is expected for a linear model, which assumes a straightforward linear relationship between variables and struggles to model non-linear or complex patterns in the data.\n",
    "- **_Difference in Predictive Power:_** The Random Forest model shows a closer alignment with the actual values, suggesting it has a higher predictive power and a lower prediction error compared to the Linear Regression model for this specific dataset.\n",
    "_This is a common finding, as Random Forest models, which are a type of ensemble learning, are often more flexible and can handle more complex relationships than linear models._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539de897-85dd-4fa9-af83-a11940cd08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Next-Day Forecast with Category + Recommendations ===\n",
    "\n",
    "def pollution_category(pm25):\n",
    "    if pm25 <= 12:\n",
    "        return \"Good\"\n",
    "    elif pm25 <= 35.4:\n",
    "        return \"Moderate\"\n",
    "    elif pm25 <= 55.4:\n",
    "        return \"Unhealthy for Sensitive Groups\"\n",
    "    elif pm25 <= 150.4:\n",
    "        return \"Unhealthy\"\n",
    "    elif pm25 <= 250.4:\n",
    "        return \"Very Unhealthy\"\n",
    "    else:\n",
    "        return \"Hazardous\"\n",
    "\n",
    "def recommendations(category):\n",
    "    if category == \"Good\":\n",
    "        return [\"Air quality is satisfactory.\", \"Enjoy outdoor activities freely.\"]\n",
    "    elif category == \"Moderate\":\n",
    "        return [\"Air quality is acceptable.\", \"Sensitive individuals should limit prolonged outdoor exertion.\"]\n",
    "    elif category == \"Unhealthy for Sensitive Groups\":\n",
    "        return [\"Sensitive groups should reduce outdoor activity.\", \"Consider wearing a mask if outdoors.\"]\n",
    "    elif category == \"Unhealthy\":\n",
    "        return [\"Everyone may begin to experience health effects.\", \"Limit outdoor activities.\", \"Use air purifiers indoors.\"]\n",
    "    elif category == \"Very Unhealthy\":\n",
    "        return [\"Health alert: everyone may experience serious effects.\", \"Avoid outdoor activity.\", \"Keep windows closed.\"]\n",
    "    else:  # Hazardous\n",
    "        return [\"Emergency conditions: serious health effects for all.\", \"Stay indoors with filtered air.\", \"Follow local advisories.\"]\n",
    "\n",
    "#lag only features for next day\n",
    "last_row = df_model.iloc[-1]\n",
    "pm25_t = last_row[\"pm25\"]\n",
    "pm25_t_minus_6 = df_model.iloc[-7][\"pm25\"] if len(df_model) >= 7 else np.nan\n",
    "pm25_roll7_tplus1 = df_model[\"pm25\"].iloc[-7:].mean()\n",
    "\n",
    "lag_next = pd.DataFrame([{\n",
    "    \"pm25_lag1\": pm25_t,\n",
    "    \"pm25_lag7\": pm25_t_minus_6,\n",
    "    \"pm25_rolling7\": pm25_roll7_tplus1\n",
    "}])\n",
    "\n",
    "#RF on lag-only features\n",
    "rf_lag = RandomForestRegressor(n_estimators=300, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "X_lag = df_model[[\"pm25_lag1\",\"pm25_lag7\",\"pm25_rolling7\"]].dropna()\n",
    "y_lag = df_model.loc[X_lag.index, \"pm25\"]\n",
    "rf_lag.fit(X_lag, y_lag)\n",
    "\n",
    "# Forecast next day\n",
    "forecast_date = pd.to_datetime(df_model[\"date\"].iloc[-1]) + pd.Timedelta(days=1)\n",
    "next_day_pred = rf_lag.predict(lag_next)[0] if not lag_next.isna().any().any() else np.nan\n",
    "\n",
    "# Classify + recommend\n",
    "category = pollution_category(next_day_pred)\n",
    "recs = recommendations(category)\n",
    "\n",
    "print(f\"Forecast date: {forecast_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Predicted PM2.5: {next_day_pred:.1f} µg/m³\")\n",
    "print(f\"Pollution category: {category}\")\n",
    "print(\"Recommendations:\")\n",
    "for r in recs:\n",
    "    print(f\" - {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98fd64-1302-4a1e-88b5-34203dd5cf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (homl3)",
   "language": "python",
   "name": "homl3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
